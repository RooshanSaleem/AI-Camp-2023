{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Translation"
      ],
      "metadata": {
        "id": "s2-VTVb23pxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 1: Using Transformers Pipeline API\n"
      ],
      "metadata": {
        "id": "6mpqbLPQypJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the `transformers` library if you haven't already:\n"
      ],
      "metadata": {
        "id": "WRVBwvmhyGJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrbT2uvfyIj5",
        "outputId": "92e8d13e-cb00-4229-b5af-9560b828dfe2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the necessary modules.\n",
        "\n",
        "The transformers library has pipeline APIs that offer a user-friendly method to utilize pre-trained models for particular tasks. These APIs take care of all the essential processes, such as tokenization, model loading, prediction, and decoding. By abstracting the underlying complexities, the pipeline APIs make it simpler to employ pre-trained models for various tasks, including text classification, named entity recognition, machine translation, and others."
      ],
      "metadata": {
        "id": "3zRkLu91yN2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "sDKWuraryMxC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the task and the model:"
      ],
      "metadata": {
        "id": "XVqk5MOT6u7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"translation_en_to_fr\"\n",
        "model = \"t5-base\""
      ],
      "metadata": {
        "id": "uJvLyHbL6xnp"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the pre-trained machine translation pipeline:"
      ],
      "metadata": {
        "id": "L-adid6fyS4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(task = task, model = model)"
      ],
      "metadata": {
        "id": "WrAgCcsRyUQ6"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Provide an input sentence or a list of sentences:"
      ],
      "metadata": {
        "id": "W1hNAfJayXnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"Large Language Models are amazing and highly versatile\", \"This presentation was very boring\"]"
      ],
      "metadata": {
        "id": "k8Uh-lepyZVL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform translation and print the results:"
      ],
      "metadata": {
        "id": "7NhSP2yIycqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translation = translator(sentences)\n",
        "for t in translation:\n",
        "  print(t['translation_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJM52yELyeaO",
        "outputId": "d48c5c87-5bb1-422b-9ef5-487e664c94c9"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les grands modèles linguistiques sont étonnants et très polyvalents\n",
            "Cette présentation était très ennuyeuse.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 2: Using AutoModelForSeq2Seq with a Pre-trained Model"
      ],
      "metadata": {
        "id": "7dzprCs2ysfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the necessary modules\n",
        "\n",
        "The transformers library includes the class **AutoTokenizer**, which selects and loads the suitable tokenizer for a given pre-trained model automatically. Tokenizers are critical in NLP tasks since they convert raw text into tokens, which can be words, subwords, or characters, depending on the tokenizer's approach. They handle text normalization, tokenization, and special token addition, and prepare the input data for the model by converting text into a format that it can process.\n",
        "\n",
        "The transformers library also includes the class **AutoModelForSeq2SeqLM**, which automatically selects and loads the appropriate pre-trained model for sequence-to-sequence tasks, such as text summarization or machine translation, abstracting the details of model loading and providing a standardized interface for using pre-trained models for sequence generation tasks.\n",
        "\n",
        "A **sequence-to-sequence** task is a type of NLP task where the input and output are both sequences of arbitrary lengths. In this scenario, the model takes a variable-length input sequence and generates an output sequence of variable length."
      ],
      "metadata": {
        "id": "Ed1Vgymjyt4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "nFMd7bbgy3lr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load a pre-trained model and its tokenizer:\n",
        "\n",
        "Helsinki-NLP/opus-mt-en-fr is a machine translation model developed by the Helsinki NLP research group. This model is specifically trained for translating text from English (en) to French (fr). It belongs to the Open Parallel Universal Sentence Encoder (OPUS) project, which aims to provide high-quality machine translation models for various language pairs."
      ],
      "metadata": {
        "id": "EubPRTbDyvHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hXR93p-y6Pr",
        "outputId": "e3e2a77f-edfe-4857-b0b2-560725cc7163"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize the input text (we will use the same sentences as before):"
      ],
      "metadata": {
        "id": "IFl93yI7yyMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.batch_encode_plus(sentences, return_tensors=\"pt\", padding=True)"
      ],
      "metadata": {
        "id": "vVzOPMS4y93R"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate the translation:"
      ],
      "metadata": {
        "id": "JbCw-gW_yztV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translation_ids = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=128)"
      ],
      "metadata": {
        "id": "m-Tionncy_Dy"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the tokenizer generates tokens, they are assigned unique numerical identifiers called **input IDs**. These IDs are crucial in helping the model understand and process the input text. Input IDs are typically used to create tensor representations of the input text, which can be fed into the model for inference.\n",
        "\n",
        "**Attention masks** are binary tensors that indicate which tokens the model should focus on and which ones to disregard. It helps the model differentiate between actual tokens and padding tokens, guaranteeing that the model only attends to the relevant parts of the input. The shape of the attention mask is the same as the input IDs tensor, and it contains 1s for real tokens and 0s for padding tokens."
      ],
      "metadata": {
        "id": "JMcXRs829U_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decode the translation tokens:"
      ],
      "metadata": {
        "id": "vIuT_4mFy075"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translations = tokenizer.batch_decode(translation_ids, skip_special_tokens=True)\n",
        "\n",
        "for sentence, translation in zip(sentences, translations):\n",
        "    print(\"Input:\", sentence)\n",
        "    print(\"Translation:\", translation)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEEg_aSdzAY0",
        "outputId": "cfacf64b-d389-4759-8f14-649e55fb0106"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Large Language Models are amazing and highly versatile\n",
            "Translation: Les modèles de grande langue sont étonnants et très polyvalents\n",
            "\n",
            "Input: This presentation was very boring\n",
            "Translation: Cette présentation était très ennuyeuse\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The pipeline API is more convenient for quick translation tasks, while the generic approach offers more control over the translation process.\n",
        "\n",
        "Note: Replace \"translation_en_to_fr\" and \"Helsinki-NLP/opus-mt-en-fr\" with the desired translation model and language pairs according to your requirements.\n",
        "\n",
        "Check the full list here: https://huggingface.co/models?pipeline_tag=translation\n"
      ],
      "metadata": {
        "id": "LJoyoVpbzGeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "PFJpaFaF3s2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distilbert-base-uncased-finetuned-sst-2-english model is based on the DistilBERT architecture, which is a smaller and faster version of the BERT (Bidirectional Encoder Representations from Transformers) model. It has been pre-trained on a large corpus of uncased English text and then fine-tuned on the Stanford Sentiment Treebank (SST-2) dataset. The SST-2 dataset consists of movie reviews labeled with their corresponding sentiment (positive or negative). By fine-tuning on this dataset, the model learns to classify the sentiment of a given English text as either positive or negative."
      ],
      "metadata": {
        "id": "k2di3AdPDWgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "encoded_inputs = tokenizer(sentences, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "input_ids = encoded_inputs[\"input_ids\"]\n",
        "attention_mask = encoded_inputs[\"attention_mask\"]\n",
        "\n",
        "outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "predicted_labels = outputs.logits.argmax(dim=1).tolist()\n",
        "\n",
        "sentiment_labels = [\"Positive\" if label == 1 else \"Negative\" for label in predicted_labels]\n",
        "\n",
        "for sentence, sentiment_label in zip(sentences, sentiment_labels):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Sentiment:\", sentiment_label)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU3bkh6C5NX_",
        "outputId": "e155448b-64db-4be7-f13f-9d47ea2c59d9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Large Language Models are amazing and highly versatile\n",
            "Sentiment: positive\n",
            "\n",
            "Sentence: This presentation was very boring\n",
            "Sentiment: negative\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After obtaining the model's outputs, the logits represent the raw scores or probabilities assigned to each class by the model. In sentiment analysis, the logits would represent the scores for positive and negative sentiment. argmax(dim=1) is used to find the index of the class with the highest score (i.e., the predicted sentiment label)."
      ],
      "metadata": {
        "id": "ikeu5Aa39fgh"
      }
    }
  ]
}